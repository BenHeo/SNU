get_map("houston", zoom = 14, color = "bw"),
extent = "device", legend = "topleft"
)
library(grid)
# only violent crimes
violent_crimes <- subset(crime,
offense != "auto theft" &
offense != "theft" &
offense != "burglary"
)
# rank violent crimes
violent_crimes$offense <-
factor(violent_crimes$offense,
levels = c("robbery", "aggravated assault",
"rape", "murder")
)
# restrict to downtown
violent_crimes <- subset(violent_crimes,
-95.39681 <= lon & lon <= -95.34188 &
29.73631 <= lat & lat <= 29.78400
)
# get map and bounding box
theme_set(theme_bw(16))
HoustonMap <- qmap("houston", zoom = 14, color = "bw",
extent = "device", legend = "topleft")
HoustonMap <- ggmap(
get_map("houston", zoom = 14, color = "bw"),
extent = "device", legend = "topleft"
)
# the bubble chart
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense, size = offense), data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# doing it with qmplot is even easier
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
)
# or, with styling:
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# a contour plot
HoustonMap +
stat_density2d(aes(x = lon, y = lat, colour = offense),
size = 3, bins = 2, alpha = 3/4, data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# 2d histogram...
HoustonMap +
stat_bin2d(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, bins = 30, alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# ... with hexagonal bins
HoustonMap +
stat_binhex(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, binwidth = c(.00225,.00225), alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# changing gears (get a color map)
houston <- get_map("houston", zoom = 14)
HoustonMap <- ggmap(houston, extent = "device", legend = "topleft")
# a filled contour plot...
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
size = 2, bins = 4, data = violent_crimes, geom = "polygon") +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))
# ... with an insert
overlay <- stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes)
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes) +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10)) +
inset(
grob = ggplotGrob(ggplot() + overlay +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
theme_inset()
),
xmin = attr(houston,"bb")$ll.lon +
(7/10) * (attr(houston,"bb")$ur.lon - attr(houston,"bb")$ll.lon),
xmax = Inf,
ymin = -Inf,
ymax = attr(houston,"bb")$ll.lat +
(3/10) * (attr(houston,"bb")$ur.lat - attr(houston,"bb")$ll.lat)
)
HoustonMap <- qmap("houston", zoom = 14, color = "bw",
extent = "device", legend = "topleft")
HoustonMap <- ggmap(
get_map("houston", zoom = 14, color = "bw"),
extent = "device", legend = "topleft"
)
# the bubble chart
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense, size = offense), data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# doing it with qmplot is even easier
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
)
# or, with styling:
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# a contour plot
HoustonMap +
stat_density2d(aes(x = lon, y = lat, colour = offense),
size = 3, bins = 2, alpha = 3/4, data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# 2d histogram...
HoustonMap +
stat_bin2d(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, bins = 30, alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# ... with hexagonal bins
HoustonMap +
stat_binhex(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, binwidth = c(.00225,.00225), alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# changing gears (get a color map)
houston <- get_map("houston", zoom = 14)
HoustonMap <- ggmap(houston, extent = "device", legend = "topleft")
# a filled contour plot...
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
size = 2, bins = 4, data = violent_crimes, geom = "polygon") +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))
# ... with an insert
overlay <- stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes)
overlay <- stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes)
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes) +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10)) +
inset(
grob = ggplotGrob(ggplot() + overlay +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
theme_inset()
),
xmin = attr(houston,"bb")$ll.lon +
(7/10) * (attr(houston,"bb")$ur.lon - attr(houston,"bb")$ll.lon),
xmax = Inf,
ymin = -Inf,
ymax = attr(houston,"bb")$ll.lat +
(3/10) * (attr(houston,"bb")$ur.lat - attr(houston,"bb")$ll.lat)
)
if (!require(sp)) {install.packages('sp'); library(sp)}
if (!require(gstat)) {install.packages('gstat'); library(gstat)}
if (!require(automap)) {install.packages('automap'); library(automap)}
if (!require(rgdal)) {install.packages('rgdal'); library(rgdal)}
if (!require(e1071)) {install.packages('e1071'); library(e1071)}
if (!require(dplyr)) {install.packages('dplyr'); library(dplyr)}
if (!require(lattice)) {install.packages('lattice'); library(lattice)}
if (!require(viridis)) {install.packages('viridis'); library(viridis)}
seoul032823 <- read.csv ("data/seoul032823.csv")
head(seoul032823)
skorea <- raster::getData(name ="GADM", country= "KOR", level=2)  # https://gadm.org/download_country_v3.html
# skorea <- readRDS("data/KOR_adm2.rds")
head(skorea,2)
if (!require(broom)) {install.packages('broom'); library(broom)}
## Loading required package: broom
skorea <- broom::tidy(skorea)
## Regions defined for each Polygons
class(skorea)
## [1] "data.frame"
head(skorea,3)
ggplot() + geom_map(data= skorea, map= skorea,
aes(map_id=id,group=group),fill=NA, colour="black") +
geom_point(data=seoul032823, aes(LON, LAT, col = PM10),alpha=0.7) +
labs(title= "PM10 Concentration in Seoul Area at South Korea",
x="Longitude", y= "Latitude", size="PM10(microgm/m3)")
coordinates(seoul032823) <- ~LON+LAT
class(seoul032823)
LON.range <- c(126.5, 127.5)
LAT.range <- c(37, 38)
seoul032823.grid <- expand.grid(
LON = seq(from = LON.range[1], to = LON.range[2], by = 0.01),
LAT = seq(from = LAT.range[1], to = LAT.range[2], by = 0.01))
plot(seoul032823.grid)
points(seoul032823, pch= 16,col="red")
coordinates(seoul032823.grid)<- ~LON+LAT ## sp class
gridded(seoul032823.grid)<- T
plot(seoul032823.grid)
points(seoul032823, pch= 16,col="red")
seoul032823_OK <- autoKrige(formula = PM10~1,
input_data = seoul032823,
new_data = seoul032823.grid )
str(seoul032823_OK)
head(seoul032823_OK$krige_output)
head(seoul032823_OK$krige_output@coords, 2)
head(seoul032823_OK$krige_output@data$var1.pred,2)
myPoints <- data.frame(seoul032823)
myKorea <- data.frame(skorea)
myKrige <- data.frame(seoul032823_OK$krige_output@coords,
pred = seoul032823_OK$krige_output@data$var1.pred)
if(!require(viridis)){install.packages("viridis"); library(viridis)}
ggplot()+ theme_minimal() +
geom_tile(data = myKrige, aes(x= LON, y= LAT, fill = pred)) +
geom_map(data= myKorea, map= myKorea, aes(map_id=id,group=group),
fill=NA, colour="black") +
coord_cartesian(xlim= LON.range ,ylim= LAT.range) +
labs(title= "PM10 Concentration in Seoul Area at South Korea",
x="Longitude", y= "Latitude")+
theme(title= element_text(hjust = 0.5,vjust = 1,face= c("bold")))+
scale_fill_viridis(option="magma")
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:7, 1:7]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 40)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1[1:7, 1:7]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', "호", "년", "것", "출처", "번", "속",
'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 30){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:7, 1:7]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 40)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1[1:7, 1:7]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', "호", "년", "것", "출처", "번", "속",
'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 30){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
which_are_related %>%
arrange(cnt)
library(tidyverse)
which_are_related %>%
arrange(cnt)
?arrange
which_are_related %>%
arrange(desc(cnt))
which_are_related %>%
arrange(desc(cnt)) %>%
head(20)
which_are_related %>%
arrange(desc(cnt)) %>%
head(50)
library(httr)
library(rvest)
if(!require(xml2)){install.packages("xml2"); library(xml2)}
service_key <- 'bq8VKx5vz8uAJmKyBCc4%2BnyKJDtYAy%2BOtSYPNSPC49wrgcxGGVlp6WxsXV%2FNKpNLm09ukA4DibxSeeIA%2BThJfA%3D%3D'
url = paste0("http://openapi.airkorea.or.kr/openapi/services/rest/",
"ArpltnInforInqireSvc/getCtprvnMesureSidoLIst?", # 시군구별 실시간 평균정보 조회
"sidoName=서울",
"&searchCondition=DAILY",
"&pageNo=",1,
"&numOfRows=",25,
"&ServiceKey=",service_key)
########################################################
readLines("test.R")
########################################################
readLines("test/test.R")
########################################################
readLines("./test/test.R")
########################################################
a = readLines("./test/test.R")
a[8]
cat(file = "test_m.R", sep = '/n')
cat(file = "./test/test_m.R", sep = '/n')
for (i in 1:8)
{
system(command = paste0("r --no-restore --no-save --args ", 10, " <test.r> test_out", i, ".txt"), wait = FALSE)
}
setwd('./test')
for (i in 1:8)
{
system(command = paste0("r --no-restore --no-save --args ", 10, " <test.r> test_out", i, ".txt"), wait = FALSE)
}
########################################################
a = readLines("test.R")
a[8] = " a = 1"
cat(file = "test_m.R", sep = '/n')
cat(file = "test_m.R", sep = '\n')
head(task_list)
# tasklist를 R을 통해 실행하고 그 중에서 R.exe가 몇 개나 실행되고 있는지 확인한다.
system("tasklist")
task_list <- system("tasklist", intern = TRUE) # intern = TRUE를 해야 저장 형태로 만들 수 있다
head(task_list)
b <-gregexpr(" ", a[[3]])[[1]]
i = 5
substring(a[[i]],1,b[1])
substring(a[[i]],b[1]+1,b[2])
gsub("(^ +)|( +$)", "", substring(a[[i]],1,b[1]))
gsub("(^ +)|( +$)", "", substring(a[[i]],b[1]+1,b[2]))
taskResult <- matrix("", length(a)-3, 5)
nn <- c("image" ,"PID" , "session_name" ,"session_num" , "memory")
colnames(taskResult) <- nn
for(i in 4L:length(a))
{
for (j in 1:5)
{
if (j == 1) tmp <- substring(a[[i]],1,b[1])
if (j == 5) tmp <- substring(a[[i]],b[4]+1)
if (j!=1 & j!=5) tmp <- substring(a[[i]],b[j-1]+1,b[j])
taskResult[i-3,j] <- gsub("(^ +)|( +$)", "", tmp)
}
}
taskResult <- matrix("", length(a)-3, 5)
nn <- c("image" ,"PID" , "session_name" ,"session_num" , "memory")
colnames(taskResult) <- nn
for(i in 4L:length(a))
{
for (j in 1:5)
{
if (j == 1) tmp <- substring(a[[i]],1,b[1])
if (j == 5) tmp <- substring(a[[i]],b[4]+1)
if (j!=1 & j!=5) tmp <- substring(a[[i]],b[j-1]+1,b[j])
taskResult[i-3,j] <- gsub("(^ +)|( +$)", "", tmp)
}
}
taskRead <- function()
{
a <- system("tasklist", intern = T)
b <-gregexpr(" ", a[[3]])[[1]]
taskResult <- matrix("", length(a)-3, 5)
nn <- c("image" ,"PID" , "session_name" ,"session_num" , "memory")
colnames(taskResult) <- nn
for(i in 4L:length(a))
{
for (j in 1:5)
{
if (j == 1) tmp <- substring(a[[i]],1,b[1])
if (j == 5) tmp <- substring(a[[i]],b[4]+1)
if (j!=1 & j!=5) tmp <- substring(a[[i]],b[j-1]+1,b[j])
taskResult[i-3,j] <- gsub("(^ +)|( +$)", "", tmp)
}
}
return(taskResult)
}
nMaxTask <-5
tr <- taskRead()
nCurrentTask  <- sum(tr[,1]=="R.exe")
nAvailableTask <- nMaxTask - nCurrentTask
