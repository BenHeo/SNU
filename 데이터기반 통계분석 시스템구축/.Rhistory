View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
save(nouns, file = "BlackPinkNouns.RData")
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# class(final_dat)
length(nouns)
nouns[[1]]
words <- c()
for (i in 1:1000){
for (word in nouns[[i]]){
words <- c(words, tolower(word))
}
}
tw <- table(words)
tw <- tw[tw>=10]
tw
hist(tw, breaks = 40, xlim = c(0, 200))
tw[tw>150]
sort(tw, decreasing = TRUE)
dfBP2 <- data.frame(tw)
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.00003, color="white", backgroundColor="pink")
save(dfBP2, file = "dfBP.RData")
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:5, 1:5]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.3, color="white", backgroundColor="pink")
?letterCloud
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.01, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.000008, color="white", backgroundColor="pink")
library(webshot)
webshot::install_phantomjs()
install.packages("webshot")
library(webshot)
webshot::install_phantomjs()
webshot::install_phantomjs()
lettercloud <- letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
# Make the graph
my_graph=lettercloud
# save it in html
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)
# and in png
webshot("tmp.html","fig_1.pdf", delay =5, vwidth = 480, vheight=480)
# Make the graph
my_graph=letterCloud(dfBP2, word = "BLACKPINK", wordSize = 3, color="white", backgroundColor="pink")
# save it in html
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)
# and in png
webshot("tmp.html","fig_1.pdf", delay =5, vwidth = 480, vheight=480)
lettercloud <- letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.3, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.1, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.1, color="white", backgroundColor="pink")
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# class(final_dat)
length(nouns)
nouns[[1]]
words <- c()
for (i in 1:1000){
for (word in nouns[[i]]){
words <- c(words, tolower(word))
}
}
tw <- table(words)
tw <- tw[tw>=5]
tw
hist(tw, breaks = 40, xlim = c(0, 200))
tw[tw>150]
sort(tw, decreasing = TRUE)
dfBP2 <- data.frame(tw)
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
dfBP2 <- data.frame(tw)
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'gt', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.1, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
wordcloud2(dfBP2, size = 2, shape = 'star')
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
which_are_related
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# class(final_dat)
length(nouns)
nouns[[1]]
words <- c()
for (i in 1:1000){
for (word in nouns[[i]]){
words <- c(words, tolower(word))
}
}
tw <- table(words)
tw <- tw[tw>=5]
tw
hist(tw, breaks = 40, xlim = c(0, 200))
tw[tw>150]
hist(tw, breaks = 10, xlim = c(0, 200))
hist(tw, breaks = 50, xlim = c(0, 200))
hist(tw, breaks = 20, xlim = c(0, 200))
hist(tw, breaks = 100, xlim = c(0, 200))
tw[tw>150]
sort(tw, decreasing = TRUE)
dfBP2 <- data.frame(tw)
library(wordcloud2)
install.packages("wordcloud2")
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
library(tidyverse)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'gt', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:5, 1:5]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
if(!require(networkD3)){install.packages("networkD3"); library(networkD3)}
if(!require(igraph)){install.packages("igraph"); library(igraph)}
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
network_list = igraph_to_networkD3(g, group = members) # igraph to d3 list
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
units = "TWh", fontSize = 18, nodeWidth = 30)
head(network_list)
head(network_list$heads)
head(network_list$links)
forceNetwork(Links = network_list$links, Nodes = network_list$nodes, NodeID = "name",
Source = "source", Target = "target",
Value = "value", arrows = T,
Group = "group", opacity = 0.8, zoom = TRUE)
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:5, 1:5]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
if(!require(networkD3)){install.packages("networkD3"); library(networkD3)}
if(!require(igraph)){install.packages("igraph"); library(igraph)}
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
network_list = igraph_to_networkD3(g, group = members) # igraph to d3 list
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:5, 1:5]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
if(!require(networkD3)){install.packages("networkD3"); library(networkD3)}
if(!require(igraph)){install.packages("igraph"); library(igraph)}
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
network_list = igraph_to_networkD3(g, group = members) # igraph to d3 list
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:5, 1:5]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
network_list = igraph_to_networkD3(g, group = members) # igraph to d3 list
head(network_list$links)
head(network_list$nodes)
network_list$nodes[network_list$links$source==109]
network_list$nodes[whichnetwork_list$links$source==109)]
network_list$nodes[which(network_list$links$source==109)]
network_list$links[network_list$links$source==109]
network_list$links$source==109
network_list$links$source[network_list$links$source==109]
network_list$nodes$names[network_list$links$source==109]
network_list$nodes$name[network_list$links$source==109]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 20){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
View(which_are_related)
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
network_list = igraph_to_networkD3(g, group = members) # igraph to d3 list
forceNetwork(Links = network_list$links, Nodes = network_list$nodes, NodeID = "name",
Source = "source", Target = "target",
Value = "value", arrows = T,
Group = "group", opacity = 0.8, zoom = TRUE)
