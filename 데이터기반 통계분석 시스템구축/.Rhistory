letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.0001, color="white", backgroundColor="pink")
save(dfBP2, file = "dfBP.RData")
dfBP2
head(word.df[word_order,]) # 합 높은 거 보기 위해
doc_list <- data_list$n_gram
uniq_words <- sort(unique(do.call('c', doc_list))) # c 통해서 vector화 해준 후, unique
count_doc <- function(word, year){
return(
sum(sapply(data_list$n_gram[data_list$date == year], function(x) word %in% x))
# 입력 받은 연도에 해당하는 n_gram들 안에 해당하는 단어의 합
)
}
word_count_2016 <- sapply(uniq_words, function(word) count_doc(word, 2016)) # 모든 unique 단어에 대해
# confidence :  사건 A와 사건 B가 동시에 발생할 확 대비 A 발생 확률 : A 교집합 B의 발생 확률 / A의 발생 확률
# conf_result 설명 : 2014~2016 confidence 평균으로 2017 년의 단어 confidence 비율 파악하려 함
conf_result <- data_list$conf_data
word_count_2016
# confidence :  사건 A와 사건 B가 동시에 발생할 확 대비 A 발생 확률 : A 교집합 B의 발생 확률 / A의 발생 확률
# conf_result 설명 : 2014~2016 confidence 평균으로 2017 년의 단어 confidence 비율 파악하려 함
conf_result <- data_list$conf_data
head(conf_result)
mean_conf <- apply(conf_result[,2:4], 1, mean) # conf_result[,2:4]의 열에 대해 평균
conf_result$increasing_rate <- conf_result[,5]/mean_conf
text_conf <- conf_result[c(which(is.finite(conf_result$increasing_rate) & conf_result$increasing_rate > 20),
which(mean_conf > 0.3)),] # 20 넘는 increasing_rate와 mean_conf 가 큰 값들에 text를 입력해주기 위해 미리 설정
loc_conf <- mean_conf[c(which(is.finite(conf_result$increasing_rate) & conf_result$increasing_rate > 20),
which(mean_conf > 0.3))]
sum(conf_result$increasing_rate == Inf) # 102개가 Inf로 나온다
plot(mean_conf, conf_result$increasing_rate, ylim = c(-0.5, 27), xlim = c(-0.01, 0.5))
text(loc_conf+0.03, text_conf$increasing_rate, labels = text_conf$word, cex = 1, pos = 3) # x좌표를 보면 위 과정 이해 될 듯
abline(h=1, col='red')
# x, y 모두 exponential하기 때문에 log scale을 취해준다
# log0은 없기 때문에 아주 작은 수를 더해준다
plot(mean_conf + 1e-4, conf_result$increasing_rate + 1e-2, log = "xy", ylim = c(1e-2, 27), xlim = c(1e-4, 2)) # log = "xy"를 해주면 xy축 모두 로그스케일
text(loc_conf*exp(0.03), text_conf$increasing_rate, labels = text_conf$word, cex = 1, pos = 3)
abline(h = 1, col = 'red')
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
doc_list = data_list$n_gram
uniq_words <- unique(do.call('c', doc_list))
occur_vec_list <- lapply(doc_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:3, 1:3]
doc_list
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:3, 1:3]
length(dtm_mat) # == nrow * ncol
dtm_mat[1:10, 1:3]
dtm_mat[1:10, 1:10]
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
nrow(dtm_mat)
ncol(dtm_mat)
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
wordcloud2(dfBP2, size = 2, shape = 'star')
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
## 앞서 구한 confidence 비율 증가가 큰 단어들을 가져온다
idx = which(conf_result$increasing_rate[which(is.finite(conf_result$increasing_rate))] >= 5) # increasing_rate가 5보다 크고 유한인 경우만 사용
co_occur_mat2 <- co_occur_mat[idx, idx]
co_occur_mat2
co_occur_mat1
nrow(co_occur_mat1)
co_occur_mat2 <- co_occur_mat1[idx, idx]
co_occur_mat2
co_occur_mat2 <- co_occur_mat1[idx, idx]
co_occur_mat2 <- co_occur_mat[idx, idx]
co_occur_mat2
co_occur_mat1
colnams(co_occur_mat1)
colnames(co_occur_mat1)
co_occur_mat1
noun_list
colnames(co_occur_mat1)
which(colnames(co_occur_mat1) == '명')
co_occur_mat1 <- co_occur_mat[-inv, -inv]
co_occur_mat1
co_occur_mat1 <- co_occur_mat[-inv, 1]
co_occur_mat1
co_occur_mat1[1:4, 1:4]
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:4, 1:4]
co_occur_mat1[-(1:4), 1:4]
co_occur_mat1[-(1:4), -(1:4)]
nrow(co_occur_mat1)
co_occur_mat1[-(5:110), -(5:110)]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
head(noIdx)
co_occur_mat1[c(2,5,7,10,12,20), c(2,5,7,10,12,20)]
co_occur_mat1[-noIdx, -noIdx]
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
View(co_occurrence)
noIdx
co_occur_mat1[1:4, 1:4]
co_occur_mat1[1:5, 1:5]
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.0001, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.00003, color="white", backgroundColor="pink")
c(c(1,2), c(3,4))
which_are_related <- data.frame()
which_are_related
which(names(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.00003, color="white", backgroundColor="pink")
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 100){
row = i
col = j
row_word = rownames(co_occurrence[i])
col_word = colnames(co_occurrence[j])
dat <- as.data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word)
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 100){
row = i
col = j
row_word = rownames(co_occurrence[i])
col_word = colnames(co_occurrence[j])
dat <- as.data.frame(row = row, col = col, row_word = row_word, col_word = col_word)
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 100){
row = i
col = j
row_word = rownames(co_occurrence[i])
col_word = colnames(co_occurrence[j])
dat <- data.frame(row = row, col = col, row_word = row_word, col_word = col_word)
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 100){
row = i
col = j
row_word = rownames(co_occurrence[i])
col_word = colnames(co_occurrence[j])
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word)
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
row = 1
col = 1
row_word = rownames(co_occurrence[1])
col_word = colnames(co_occurrence[1])
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word)
dat <- data.frame("row" = row)
dat
dat <- data.frame("row" = row, "col" = col)
dat
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 100){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word)
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 100){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i != j){
if (co_occurrence[i,j] > 50){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i <= j){
if (co_occurrence[i,j] > 50){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 50){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 30){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 10){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
View(which_are_related)
save(nouns, file = "BlackPinkNouns.RData")
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# class(final_dat)
length(nouns)
nouns[[1]]
words <- c()
for (i in 1:1000){
for (word in nouns[[i]]){
words <- c(words, tolower(word))
}
}
tw <- table(words)
tw <- tw[tw>=10]
tw
hist(tw, breaks = 40, xlim = c(0, 200))
tw[tw>150]
sort(tw, decreasing = TRUE)
dfBP2 <- data.frame(tw)
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.00003, color="white", backgroundColor="pink")
save(dfBP2, file = "dfBP.RData")
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:4, 1:4]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 30)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1
co_occur_mat1[1:5, 1:5]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', 'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 15){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.3, color="white", backgroundColor="pink")
?letterCloud
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.01, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.000008, color="white", backgroundColor="pink")
library(webshot)
webshot::install_phantomjs()
install.packages("webshot")
library(webshot)
webshot::install_phantomjs()
webshot::install_phantomjs()
lettercloud <- letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
# Make the graph
my_graph=lettercloud
# save it in html
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)
# and in png
webshot("tmp.html","fig_1.pdf", delay =5, vwidth = 480, vheight=480)
# Make the graph
my_graph=letterCloud(dfBP2, word = "BLACKPINK", wordSize = 3, color="white", backgroundColor="pink")
# save it in html
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)
# and in png
webshot("tmp.html","fig_1.pdf", delay =5, vwidth = 480, vheight=480)
lettercloud <- letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.8, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.3, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.1, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.1, color="white", backgroundColor="pink")
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# class(final_dat)
length(nouns)
nouns[[1]]
words <- c()
for (i in 1:1000){
for (word in nouns[[i]]){
words <- c(words, tolower(word))
}
}
tw <- table(words)
tw <- tw[tw>=5]
tw
hist(tw, breaks = 40, xlim = c(0, 200))
tw[tw>150]
sort(tw, decreasing = TRUE)
dfBP2 <- data.frame(tw)
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
dfBP2 <- data.frame(tw)
library(wordcloud2)
# wordcloud2(demoFreq, size = 1,shape = 'star')
# ======================================================================================
dfBP2 <- dfBP2 %>%
filter(!(words %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시')))
dfBP2 <- dfBP2 %>%
filter(!(words %in% c('amp', 'gt', 'k', 'lt', 'm',  'q', 's', 'x', 'v', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd')))
wordcloud2(dfBP2, size = 2, shape = 'star')
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.1, color="white", backgroundColor="pink")
letterCloud(dfBP2, word = "BLACKPINK", wordSize = 0.008, color="white", backgroundColor="pink")
wordcloud2(dfBP2, size = 2, shape = 'star')
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
which_are_related
