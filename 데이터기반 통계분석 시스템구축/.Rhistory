}
return(1-(mul/365^n))
}
bp(2)
bp(3)
print(mul)
return(1-(mul/365^n))
bp <- function(n)
{
a <- (365-n+1:365)
mul <- 1
b <- for (i in a){
mul = mul*i
}
print(mul)
return(1-(mul/365^n))
}
bp(3)
bp <- function(n)
{
a <- (365-n+1:365)
print(a)
mul <- 1
b <- for (i in a){
mul = mul*i
}
print(mul)
return(1-(mul/365^n))
}
bp(3)
bp <- function(n)
{
a <- (365-n+1):365
print(a)
mul <- 1
b <- for (i in a){
mul = mul*i
}
print(mul)
return(1-(mul/365^n))
}
bp(3)
bp <- function(n)
{
a <- (365-n+1):365
mul <- 1
b <- for (i in a){
mul = mul*i
}
return(1-(mul/365^n))
}
bp(3)
for (i in 1:100)
for (i in 1:100)
bp(13)
for (i in 2:100)
{
if (bp(i) > 0.5)
{
return(i)
break
}
}
for (i in 2:100){
if (bp(i) > 0.5)
{
print(i)
break
}
}
bp(23)
rb <- function(a,b,c,d){
re <- (a/(a+b)) + (c/(c+d))
return(re/2)
}
rb(99,1,1,1)
1/365
bp(2)
bp(3)
10/17
pbinom(3, 30, 0.05)
1-pbinom(26,30,0.95)
1/0.091
if(!require(rvest)){install.packages('rvest') ; library(rvest)}
url_tvcast = 'http://tvcast.naver.com/jtbc.youth'
html_tvcast = read_html(x = url_tvcast, encoding = 'UTF-8')
html_tvcast %>%
html_nodes(".title a") %>% # title 안에 a가 있는 경우만 찾아내겠다
head(n = 3)
# tag를 다 떼고 text만 불러오려면 html_text
html_tvcast %>%
html_nodes(".title a") %>%
html_text() %>%
head(3)
# 왕은 사랑한다
url_tvcast2 = 'http://tv.naver.com/mbc.kingloves'
html_tvcast2 = read_html(x = url_tvcast2, encoding = 'UTF-8')
html_tvcast2 %>%
html_nodes(".title a") %>%
html_text() %>%
head(3)
html_tvcast2 %>%
html_nodes(".title a") %>%
html_text() %>%
data.frame() %>%
head(3)
html_tvcast %>%
html_nodes("body.ch_home #u_skip a") %>%
head(3)
html_tvcast %>%
html_nodes("body.ch_home #u_skip a") %>%
head_text() %>%
head(3)
html_tvcast %>%
html_nodes("body.ch_home #u_skip a") %>%
html_text() %>%
head(3)
# t distribution
url_wiki = 'https://en.wikipedia.org/wiki/Student%27s_t-distribution'
html_wiki = read_html(x=url_wiki, encoding = 'UTF-8')
html_wiki %>%
html_nodes('.wikitable') %>%
html_table() %>%
data.frame() %>%
head(5)
# MLB
url <- "http://www.baseball-reference.com/leagues/MLB/2017.shtml"
webpage <- read_html(url)
webpage %>%
html_nodes('div #div_teams_standard_batting table') %>%
html_table() %>%
data.frame() %>%
head(5)
batting_table <- vector("list", length(years))
names(batting_table) <- years
cat(i, "\n")
for (i in 1:length(years)){
url <- paste0(mlb_base_url, years[i],".shtml")
webpage <- read_html(url)
batting_table[[i]] <- webpage %>%
html_nodes('div #div_teams_standard_batting table') %>%
html_table() %>%
data.frame()
batting_table[[i]] <- batting_table[[i]][1:(nrow(batting_table[[i]])-3),]
batting_table[[i]][-1] <- Map(as.numeric, batting_table[[i]][-1])
cat(i, "\n")
}
# 50년치를 모으자
mlb_base_url <- "http://www.baseball-reference.com/leagues/MLB/"
years <- 2008:2017
batting_table <- vector("list", length(years))
names(batting_table) <- years
for (i in 1:length(years)){
url <- paste0(mlb_base_url, years[i],".shtml")
webpage <- read_html(url)
batting_table[[i]] <- webpage %>%
html_nodes('div #div_teams_standard_batting table') %>%
html_table() %>%
data.frame()
batting_table[[i]] <- batting_table[[i]][1:(nrow(batting_table[[i]])-3),]
batting_table[[i]][-1] <- Map(as.numeric, batting_table[[i]][-1])
cat(i, "\n")
}
webpage %>%
html_nodes('div #div_teams_standard_batting table') %>%
html_table() %>%
data.frame() %>%
tail(5)
webpage
temp <- webpage %>%
html_nodes('div #div_teams_standard_batting table') %>%
html_table() %>%
data.frame() %>%
head(5)
temp
temp[-1]
# 기상청 데이터
url = "http://www.weather.go.kr/weather/observation/currentweather.jsp?auto_man=m&type=t99&tm=2017.09.06.13%3A00&x=19&y=3"
webpage <- read_html(url, encoding = "EUC-KR")
Sys.setlocale("LC_ALL", "English")
?setlocale
?Sys.setlocale
webpage %>% html_nodes("table.table_develop3")
tmp <- webpage %>% html_nodes("table.table_develop3") %>%
html_table(header = FALSE, fill=TRUE)%>%
data.frame()
head(tmp)
Sys.setlocale("LC_ALL", "Korean")
tmp
for(i in 1:ncol(tmp)){
tmp[,i] = rvest::repair_encoding(tmp[,i])
}
head(tmp)
query = '새우깡'
query
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
query
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query
query = toupper(query)
query
end_num = 1000
display_num = 100
start_point = seq(1,end_num,display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',
query,'&display=',display_num,'&start=',
start_point[i],'&sort=sim')
url
url_body = read_xml(GET(url, header))
if(!require(httr)){install.packages("httr"); library(httr)}
url_body = read_xml(GET(url, header))
# API
client_id = 'ZoVED2Kc25huNZGJFpev';
client_secret = 'ZoVED2Kc25huNZGJFpev';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
url_body = read_xml(GET(url, header))
url_body = read_xml(GET(url, header))
url_body = read_xml(GET(url, header))
client_secret = 'JGw2VkcxyA';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
url_body = read_xml(GET(url, header))
title = url_body %>%
xml_nodes('item title') %>%
xml_text()
head(title)
bloggername = url_body %>%
xml_nodes('item bloggername') %>%
xml_text()
bloggername
link = url_body %>%
xml_nodes('item link') %>%
xml_text()
head(link)
description = url_body %>%
xml_nodes('item description') %>%
xml_text()
head(description)
require(httr)
header
nrow(final_dat)
end_num = 1000
display_num = 100
start_point = seq(1,end_num,display_num) # 이와 같이 해주는 이유는 display를 100개까지 밖에 한 번에 못 긁기 때문이다
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
# query를 치고 100개씩 하겠다고 알리며, 시작은 start_point[i]로 하겠다는 의미
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
# bloggername 같은 것은 네이버에서[https://developers.naver.com/docs/search/blog/] 제공한다
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
nrow(final_dat)
rand_row = runif(60, 1, 1000)
final_dat[rand_row]
final_dat[rand_row,]
ncol(final_dat)
rand_row
rand_row = int(runif(60, 1, 1000))
rand_row = as.integer(runif(60, 1, 1000))
rand_row
final_dat[rand_row]
final_dat[rand_row,]
# 네이버 영화 평점
### gsub는 매칭되는 것 다 찾아서 원하는 것으로 바꿈
movie_url = paste0('https://movie.naver.com/movie/point/af/list.nhn?&page=1')
movie_html = read_html(GET(movie_url), encoding = 'CP949')
contents = html_nodes(movie_html, '.title') %>%
html_text()
contents
contents = gsub('\n\t|<.*?>|&quot;', '', contents)
contents
contents = gsub('\n|\t|<.*?>|&quot;', '', contents)
contents
# 네이버 영화 평점
### gsub는 매칭되는 것 다 찾아서 원하는 것으로 바꿈
total_con = NULL
for i in 1:10{
movie_url = paste0('https://movie.naver.com/movie/point/af/list.nhn?&page=', i)
movie_html = read_html(GET(movie_url), encoding = 'CP949')
contents = html_nodes(movie_html, '.title') %>%
html_text() # 이대로 출력하면 \r\n\t 같은 것이 매우 많다
contents = gsub('\n|\t|<.*?>|&quot;', '', contents) # 이렇게 하면 \r 이랑 신고 외에는 대부분 정상적이다
part_con = data.frame(do.call(rbind,
lapply(strsplit(contents, '\r'),
function(x){x[x != "" & x != "신고"]})))
total_con = rbind(total_con, part_con)
cat(i, "\n")
}
# 네이버 영화 평점
### gsub는 매칭되는 것 다 찾아서 원하는 것으로 바꿈
total_con = NULL
for (i in 1:10) {
movie_url = paste0('https://movie.naver.com/movie/point/af/list.nhn?&page=', i)
movie_html = read_html(GET(movie_url), encoding = 'CP949')
contents = html_nodes(movie_html, '.title') %>%
html_text() # 이대로 출력하면 \r\n\t 같은 것이 매우 많다
contents = gsub('\n|\t|<.*?>|&quot;', '', contents) # 이렇게 하면 \r 이랑 신고 외에는 대부분 정상적이다
part_con = data.frame(do.call(rbind,
lapply(strsplit(contents, '\r'),
function(x) {x[x != "" & x != "신고"]} )))
total_con = rbind(total_con, part_con)
cat(i, "\n")
}
head(total_con)
?do.call
## 글 + 평점 크롤링
total_star = NULL
for(i in 1:10){
url = paste0("https://movie.naver.com/movie/point/af/list.nhn?&page=",i)
mov_html = read_html(GET(url), encoding = "CP949")
content = html_nodes(mov_html, '.title') %>% html_text()
content = gsub('\n|\t|<.*?>|&quot;','',content)
point = html_nodes(mov_html, '.point') %>% html_text()
part_star = data.frame(do.call(rbind,
lapply(strsplit(content, "\r"), function(x) {x[x != "" & x != "신고"]})), point = point)
total_star = rbind(total_dat, part_star)
cat(i, "\n")
}
## 글 + 평점 크롤링
total_star = NULL
for(i in 1:10){
url = paste0("https://movie.naver.com/movie/point/af/list.nhn?&page=",i)
mov_html = read_html(GET(url), encoding = "CP949")
content = html_nodes(mov_html, '.title') %>% html_text()
content = gsub('\n|\t|<.*?>|&quot;','',content)
point = html_nodes(mov_html, '.point') %>% html_text()
part_star = data.frame(do.call(rbind,
lapply(strsplit(content, "\r"), function(x) {x[x != "" & x != "신고"]})), point = point)
total_star = rbind(total_star, part_star)
cat(i, "\n")
}
head(total_star)
# 네이버 영화 평점
### gsub는 매칭되는 것 다 찾아서 원하는 것으로 바꿈
## 평가 글 크롤링
total_con = NULL
for (i in 1:10) {
movie_url = paste0('https://movie.naver.com/movie/point/af/list.nhn?&page=', i)
movie_html = read_html(GET(movie_url), encoding = 'UTF-8')
contents = html_nodes(movie_html, '.title') %>%
html_text() # 이대로 출력하면 \r\n\t 같은 것이 매우 많다
contents = gsub('\n|\t|<.*?>|&quot;', '', contents) # \n \t <br> "" '' 같이 표현된 모든 것을 제거
# 이렇게 하면 \r 이랑 신고 외에는 대부분 정상적이다
part_con = data.frame(do.call(rbind,
lapply(strsplit(contents, '\r'), # \r 로 split한다
function(x) {x[x != "" & x != "신고"]} ))) # 빈 것과 신고는 지운다
total_con = rbind(total_con, part_con)
cat(i, "\n")
}
# 네이버 영화 평점
### gsub는 매칭되는 것 다 찾아서 원하는 것으로 바꿈
## 평가 글 크롤링
total_con = NULL
for (i in 1:10) {
movie_url = paste0('https://movie.naver.com/movie/point/af/list.nhn?&page=', i)
movie_html = read_html(GET(movie_url), encoding = 'CP949')
contents = html_nodes(movie_html, '.title') %>%
html_text() # 이대로 출력하면 \r\n\t 같은 것이 매우 많다
contents = gsub('\n|\t|<.*?>|&quot;', '', contents) # \n \t <br> "" '' 같이 표현된 모든 것을 제거
# 이렇게 하면 \r 이랑 신고 외에는 대부분 정상적이다
part_con = data.frame(do.call(rbind, # do.call은 list에 있는 내용에 대해 특정 function을 수행해주는 것
lapply(strsplit(contents, '\r'), # \r 로 split한다
function(x) {x[x != "" & x != "신고"]} ))) # 빈 것과 신고는 지운다
total_con = rbind(total_con, part_con)
cat(i, "\n")
}
## 글 + 평점 크롤링
total_star = NULL
for(i in 1:10){
url = paste0("https://movie.naver.com/movie/point/af/list.nhn?&page=",i)
mov_html = read_html(GET(url), encoding = "CP949")
content = html_nodes(mov_html, '.title') %>% html_text()
content = gsub('\n|\t|<.*?>|&quot;','',content)
point = html_nodes(mov_html, '.point') %>% html_text()
part_star = data.frame(do.call(rbind,
lapply(strsplit(content, "\r"), function(x) {x[x != "" & x != "신고"]})), point = point)
total_star = rbind(total_star, part_star)
cat(i, "\n")
}
head(total_star)
pnorm(2)
_-5
pnorm(2)-0.5
(pnorm(2)-0.5)*2
?graph.adjacency
??graph.adjacency
?igraph::simplify
?igraph::cluster_walktrap
?igraph::membership
members
source('C:/Users/renz/Desktop/SNU/데이터기반 통계분석 시스템구축/Network.R', encoding = 'UTF-8', echo=TRUE)
getwd()
setwd('C:\Users\renz\Desktop\SNU\데이터기반 통계분석 시스템구축')
setwd('C:/Users/renz/Desktop/SNU/데이터기반 통계분석 시스템구축')
source('C:/Users/renz/Desktop/SNU/데이터기반 통계분석 시스템구축/Network.R', encoding = 'UTF-8', echo=TRUE)
?igraph::cluster_fast_greedy
network_list = cluster_fast_greedy(g)
network_list$links$group = network_list$nodes$group[network_list$links$source+1]
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
NodeGroup = "group", LinkGroup = "group",
units = "TWh", fontSize = 18, nodeWidth = 30)
network_list = cluster_fast_greedy(g, group = members)
head(network_list)
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
head(g)
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
head(wc)
head(members)
network_list = cluster_fast_greedy(g, members)
network_list$links$group = network_list$nodes$group[network_list$links$source+1]
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
NodeGroup = "group", LinkGroup = "group",
units = "TWh", fontSize = 18, nodeWidth = 30)
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
NodeGroup = "group", LinkGroup = "group",
units = "TWh", fontSize = 18, nodeWidth = 30)
# 연결관계끼리 관계선을 연결성 느껴지게 그리기. 선후 관계가 없는 관계인데 그냥 먼저 나오는 것의 색을 유지
network_list = igraph_to_networkD3(g, group = as.character(members))
network_list$links$group = network_list$nodes$group[network_list$links$source+1]
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
NodeGroup = "group", LinkGroup = "group",
units = "TWh", fontSize = 18, nodeWidth = 30)
network_list = igraph_to_networkD3(cluster_fast_greedy(g))
network_list$links$group = network_list$nodes$group[network_list$links$source+1]
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
NodeGroup = "group", LinkGroup = "group",
units = "TWh", fontSize = 18, nodeWidth = 30)
# 연결관계끼리 관계선을 연결성 느껴지게 그리기. 선후 관계가 없는 관계인데 그냥 먼저 나오는 것의 색을 유지
network_list = igraph_to_networkD3(g, group = as.character(members))
network_list$links$group = network_list$nodes$group[network_list$links$source+1]
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
NodeGroup = "group", LinkGroup = "group",
units = "TWh", fontSize = 18, nodeWidth = 30)
class(g)
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
class(g)
class(network_list)
g = graph.adjacency(co_occur_mat1, weighted = T, mode = 'undirected') # 인접행렬 형태에서 igraph 만들기 편하게 해주는 함수
g = simplify(g) # loop나 다중간선 없게
wc = cluster_walktrap(g) # communities(densely connected subgraphs) 찾기
members = membership(wc)
network_list = igraph_to_networkD3(g, group = members) # igraph to d3 list
sankeyNetwork(Links = network_list$links, Nodes = network_list$nodes,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
units = "TWh", fontSize = 18, nodeWidth = 30)
# circular plot
if(!require(circlize)){install.packages("circlize"); library(circlize)}
name=c(3,10,10,3,6,7,8,3,6,1,2,2,6,10,2,3,3,10,4,5,9,10)
feature=paste("feature ", c(1,1,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,5,5,5,5),
sep="")
dat <- data.frame(name,feature)
head(dat)
dat <- data.frame(name,feature)
dat <- table(name, feature)
head(dat,4)
chordDiagram(as.data.frame(dat), transparency = 0.5)
class(dat)
typeof(dat)
library(chorddiag)}
if(!require(RColorBrewer)){install.packages("RColorBrewer")}
if(!require(chorddiag)){devtools::install_github("mattflor/chorddiag"); # should install devtools package
library(chorddiag)}
doc_list = data_list$n_gram
table_list = lapply(doc_list, table)[1:3]
table_name = unique(unlist(do.call("c", doc_list[1:3] )))
names(table_list) = paste0("doc_", 1:3)
table_list = lapply(table_list, function(x){ # 테이블 만들 때 빈 자료란은 0으로 채운다
word_table = rep(0, length = length(uniq_words))
word_table = ifelse(uniq_words %in% names(x), x, 0)
}
)
table_list = do.call("rbind", table_list)
refined_table_list = t(table_list[, apply(table_list, 2, sum) != 0])
rownames(refined_table_list) = table_name
groupColors <- brewer.pal(3, "Set3")
chorddiag(refined_table_list
, groupColors = groupColors,  type = "bipartite", tickInterval = 3
,groupnameFontsize = 15)
