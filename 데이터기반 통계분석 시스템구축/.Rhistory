autoplot(map_p)
plot(map_p)
abline(h = 38, col = 'blue')
map_p <- openproj(map1, projection =
CRS("+proj=utm +zone=52N + datum=WGS84"))
plot(map_p)
abline(h = 38, col = 'blue')
str(map_p)
# 이제 map_p에 line을 그을 수 없다. 그렇다면 점을 찍어서 잇자
## 데이터 프레임 만들기
a <- data.frame(lon = seq(100, 140, by=0.1), lat = 38); head(a)
## sp 클래스로 변환하기
sp::coordinates(a) = ~ lon + lat; head(a) # a has coords, bbox
str(a)
abline(h = 38, col = 'blue')
str(map_p)
# 이제 map_p에 line을 그을 수 없다. 그렇다면 점을 찍어서 잇자
## 데이터 프레임 만들기
a <- data.frame(lon = seq(100, 140, by=0.1), lat = 38); head(a)
## sp 클래스로 변환하기
sp::coordinates(a) = ~ lon + lat; head(a) # a has coords, bbox
str(a)
a@coords # slot 열 때는 @로 한다. $와 비슷한 역할
## projection을 설정하기
sp::proj4string(a) = "+proj=longlat" # map_p와 같게
str(a)
## 좌표계 변환하기
a_tf = spTransform(a,  CRS("+proj=utm +zone=52N + datum=WGS84"))  # map_p와 같게
str(a_tf)
# 지도위에 매핑하기
plot(map_p)
points(a_tf@coords[,1], a_tf@coords[,2], type='l', col='blue') # lon, lat으로 점 그리기 type = 'l'
# 지도 위에 파이차트 그리기
library(mapplots) # add.pie를 위해
map = openmap(upperLeft = c(43, 119),lowerRight = c(33, 134))
seoul_loc = geocode('Seoul') # 서울의 geocode를 받아옴(lon, lat)
coordinates(seoul_loc) = ~lon + lat # @coords를 lon과 lat 데이터로
proj4string(seoul_loc) = "+proj=longlat +datum=WGS84"
seoul_loc_Tf = spTransform(seoul_loc,
CRS(as.character(map$tiles[[1]]$projection)))
plot(map)
add.pie(z=1:2,labels = c('a','b'),
x = seoul_loc_Tf@coords[1],
y = seoul_loc_Tf@coords[2], radius = 100000)
# 범죄 데이터
data(crime)
head(crime, 3)
violent_crimes = subset(crime,
offense != "auto theft" &
offense != "theft" &
offense != "burglary")
violent_crimes$offense <- factor(violent_crimes$offense,
levels = c("robbery", "aggravated assault", "rape", "murder"))
violent_crimes = subset(violent_crimes,
-95.39681 <= lon & lon <= -95.34188 &
29.73631 <= lat & lat <=  29.78400)
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft")
HoustonMap + geom_point(aes(x = lon, y = lat, colour = offense, size = offense),
xlim = c(-95.39681, -95.34188),data = violent_crimes)
install.packages("panel.margin")
install.packages("panel.spacing")
install.packages("panel")
library(ggmap)
violent_crimes = subset(crime,
offense != "auto theft" &
offense != "theft" &
offense != "burglary")
violent_crimes$offense <- factor(violent_crimes$offense,
levels = c("robbery", "aggravated assault", "rape", "murder"))
violent_crimes = subset(violent_crimes,
-95.39681 <= lon & lon <= -95.34188 &
29.73631 <= lat & lat <=  29.78400)
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft")
panel.spacing
panel.margin
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft")
HoustonMap = qmap("Houston", zoom = 14, color = "bw", legend = "topleft")
HoustonMap + geom_point(aes(x = lon, y = lat, colour = offense, size = offense),
xlim = c(-95.39681, -95.34188),data = violent_crimes)
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft")
load('data/airport.Rdata')
head(airport_krjp)
map = ggmap(get_googlemap(center = c(lon=134, lat=36),
zoom = 5, maptype='roadmap', color='bw'))
map + geom_line(data=link_krjp,aes(x=lon,y=lat,group=group),
col='grey10',alpha=0.05) +
geom_point(data=airport_krjp[complete.cases(airport_krjp),],
aes(x=lon,y=lat, size=Freq), colour='black',alpha=0.5) +
scale_size(range=c(0,15))
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft")
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft", extend = "spacing")
HoustonMap = qmap("houston", zoom = 14, color = "bw", legend = "topleft", extend = "panel.spacing")
theme_set(theme_bw(16))
HoustonMap <- qmap("houston", zoom = 14, color = "bw",
extent = "device", legend = "topleft")
HoustonMap <- ggmap(
get_map("houston", zoom = 14, color = "bw"),
extent = "device", legend = "topleft"
)
library(grid)
# only violent crimes
violent_crimes <- subset(crime,
offense != "auto theft" &
offense != "theft" &
offense != "burglary"
)
# rank violent crimes
violent_crimes$offense <-
factor(violent_crimes$offense,
levels = c("robbery", "aggravated assault",
"rape", "murder")
)
# restrict to downtown
violent_crimes <- subset(violent_crimes,
-95.39681 <= lon & lon <= -95.34188 &
29.73631 <= lat & lat <= 29.78400
)
# get map and bounding box
theme_set(theme_bw(16))
HoustonMap <- qmap("houston", zoom = 14, color = "bw",
extent = "device", legend = "topleft")
HoustonMap <- ggmap(
get_map("houston", zoom = 14, color = "bw"),
extent = "device", legend = "topleft"
)
# the bubble chart
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense, size = offense), data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# doing it with qmplot is even easier
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
)
# or, with styling:
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# a contour plot
HoustonMap +
stat_density2d(aes(x = lon, y = lat, colour = offense),
size = 3, bins = 2, alpha = 3/4, data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# 2d histogram...
HoustonMap +
stat_bin2d(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, bins = 30, alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# ... with hexagonal bins
HoustonMap +
stat_binhex(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, binwidth = c(.00225,.00225), alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# changing gears (get a color map)
houston <- get_map("houston", zoom = 14)
HoustonMap <- ggmap(houston, extent = "device", legend = "topleft")
# a filled contour plot...
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
size = 2, bins = 4, data = violent_crimes, geom = "polygon") +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))
# ... with an insert
overlay <- stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes)
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes) +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10)) +
inset(
grob = ggplotGrob(ggplot() + overlay +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
theme_inset()
),
xmin = attr(houston,"bb")$ll.lon +
(7/10) * (attr(houston,"bb")$ur.lon - attr(houston,"bb")$ll.lon),
xmax = Inf,
ymin = -Inf,
ymax = attr(houston,"bb")$ll.lat +
(3/10) * (attr(houston,"bb")$ur.lat - attr(houston,"bb")$ll.lat)
)
HoustonMap <- qmap("houston", zoom = 14, color = "bw",
extent = "device", legend = "topleft")
HoustonMap <- ggmap(
get_map("houston", zoom = 14, color = "bw"),
extent = "device", legend = "topleft"
)
# the bubble chart
HoustonMap +
geom_point(aes(x = lon, y = lat, colour = offense, size = offense), data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# doing it with qmplot is even easier
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
)
# or, with styling:
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite",
color = offense, size = offense, legend = "topleft"
) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
scale_size_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder"),
range = c(1.75,6)) +
guides(size = guide_legend(override.aes = list(size = 6))) +
theme(
legend.key.size = grid::unit(1.8,"lines"),
legend.title = element_text(size = 16, face = "bold"),
legend.text = element_text(size = 14)
) +
labs(colour = "Offense", size = "Offense")
# a contour plot
HoustonMap +
stat_density2d(aes(x = lon, y = lat, colour = offense),
size = 3, bins = 2, alpha = 3/4, data = violent_crimes) +
scale_colour_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# 2d histogram...
HoustonMap +
stat_bin2d(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, bins = 30, alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# ... with hexagonal bins
HoustonMap +
stat_binhex(aes(x = lon, y = lat, colour = offense, fill = offense),
size = .5, binwidth = c(.00225,.00225), alpha = 2/4, data = violent_crimes) +
scale_colour_discrete("Offense",
labels = c("Robery","Aggravated Assault","Rape","Murder"),
guide = FALSE) +
scale_fill_discrete("Offense", labels = c("Robery","Aggravated Assault","Rape","Murder")) +
theme(
legend.text = element_text(size = 15, vjust = .5),
legend.title = element_text(size = 15,face="bold"),
legend.key.size = grid::unit(1.8,"lines")
)
# changing gears (get a color map)
houston <- get_map("houston", zoom = 14)
HoustonMap <- ggmap(houston, extent = "device", legend = "topleft")
# a filled contour plot...
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
size = 2, bins = 4, data = violent_crimes, geom = "polygon") +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))
# ... with an insert
overlay <- stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes)
overlay <- stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes)
HoustonMap +
stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
bins = 4, geom = "polygon", data = violent_crimes) +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10)) +
inset(
grob = ggplotGrob(ggplot() + overlay +
scale_fill_gradient("Violent\nCrime\nDensity") +
scale_alpha(range = c(.4, .75), guide = FALSE) +
theme_inset()
),
xmin = attr(houston,"bb")$ll.lon +
(7/10) * (attr(houston,"bb")$ur.lon - attr(houston,"bb")$ll.lon),
xmax = Inf,
ymin = -Inf,
ymax = attr(houston,"bb")$ll.lat +
(3/10) * (attr(houston,"bb")$ur.lat - attr(houston,"bb")$ll.lat)
)
if (!require(sp)) {install.packages('sp'); library(sp)}
if (!require(gstat)) {install.packages('gstat'); library(gstat)}
if (!require(automap)) {install.packages('automap'); library(automap)}
if (!require(rgdal)) {install.packages('rgdal'); library(rgdal)}
if (!require(e1071)) {install.packages('e1071'); library(e1071)}
if (!require(dplyr)) {install.packages('dplyr'); library(dplyr)}
if (!require(lattice)) {install.packages('lattice'); library(lattice)}
if (!require(viridis)) {install.packages('viridis'); library(viridis)}
seoul032823 <- read.csv ("data/seoul032823.csv")
head(seoul032823)
skorea <- raster::getData(name ="GADM", country= "KOR", level=2)  # https://gadm.org/download_country_v3.html
# skorea <- readRDS("data/KOR_adm2.rds")
head(skorea,2)
if (!require(broom)) {install.packages('broom'); library(broom)}
## Loading required package: broom
skorea <- broom::tidy(skorea)
## Regions defined for each Polygons
class(skorea)
## [1] "data.frame"
head(skorea,3)
ggplot() + geom_map(data= skorea, map= skorea,
aes(map_id=id,group=group),fill=NA, colour="black") +
geom_point(data=seoul032823, aes(LON, LAT, col = PM10),alpha=0.7) +
labs(title= "PM10 Concentration in Seoul Area at South Korea",
x="Longitude", y= "Latitude", size="PM10(microgm/m3)")
coordinates(seoul032823) <- ~LON+LAT
class(seoul032823)
LON.range <- c(126.5, 127.5)
LAT.range <- c(37, 38)
seoul032823.grid <- expand.grid(
LON = seq(from = LON.range[1], to = LON.range[2], by = 0.01),
LAT = seq(from = LAT.range[1], to = LAT.range[2], by = 0.01))
plot(seoul032823.grid)
points(seoul032823, pch= 16,col="red")
coordinates(seoul032823.grid)<- ~LON+LAT ## sp class
gridded(seoul032823.grid)<- T
plot(seoul032823.grid)
points(seoul032823, pch= 16,col="red")
seoul032823_OK <- autoKrige(formula = PM10~1,
input_data = seoul032823,
new_data = seoul032823.grid )
str(seoul032823_OK)
head(seoul032823_OK$krige_output)
head(seoul032823_OK$krige_output@coords, 2)
head(seoul032823_OK$krige_output@data$var1.pred,2)
myPoints <- data.frame(seoul032823)
myKorea <- data.frame(skorea)
myKrige <- data.frame(seoul032823_OK$krige_output@coords,
pred = seoul032823_OK$krige_output@data$var1.pred)
if(!require(viridis)){install.packages("viridis"); library(viridis)}
ggplot()+ theme_minimal() +
geom_tile(data = myKrige, aes(x= LON, y= LAT, fill = pred)) +
geom_map(data= myKorea, map= myKorea, aes(map_id=id,group=group),
fill=NA, colour="black") +
coord_cartesian(xlim= LON.range ,ylim= LAT.range) +
labs(title= "PM10 Concentration in Seoul Area at South Korea",
x="Longitude", y= "Latitude")+
theme(title= element_text(hjust = 0.5,vjust = 1,face= c("bold")))+
scale_fill_viridis(option="magma")
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:7, 1:7]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 40)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1[1:7, 1:7]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', "호", "년", "것", "출처", "번", "속",
'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 30){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
# save(dfBP2, file = "dfBP.RData")
#============================================================================================================
load("dfBP.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
# save(nouns, file = "BlackPinkNouns.RData")
load("BlackPinkNouns.RData")
# Term Document Matrix 만들기 : 특정 Document 컬럼에 Term이 몇 번 나왔는지 나타내는 df
noun_list = nouns
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
uniq_words <- unique(do.call('c', noun_list))
occur_vec_list <- lapply(noun_list, function(x) uniq_words %in% x)
dtm_mat = do.call('rbind', occur_vec_list) # list를 matrix 형으로 바꿔줬음
colnames(dtm_mat) <- uniq_words # uniq_words들의
dtm_mat[1:7, 1:7]
length(dtm_mat) # == nrow * ncol
# nrow(dtm_mat)
# ncol(dtm_mat)
refined_dtm_mat <- dtm_mat[, colSums(dtm_mat) != 0] # 단어 중 문서 전체에서 하나도 안 나온 것은 제거
refined_dtm_mat <- refined_dtm_mat[rowSums(dtm_mat) != 0,]
co_occur_mat <- t(refined_dtm_mat) %*% refined_dtm_mat # 행렬 곱을 통해 특정 단어가 나온 문서에서 다른 특정 단어가 나온 빈도수 표현 (마코브 체인 활용)
# 나오는 결과는 단어 X 단어 matrix
co_occur_mat[1:7, 1:7]
# co_occur_mat의 숫자의 강도를 power로 주고 sankey 그래프를 그리자
# 우선 matrix 크기를 줄일 것이다
## diag의 수가 빈도를 의미하기 때문에 diag가 너무 작은 것은 제거한다
inv = (diag(co_occur_mat) >= 40)
co_occur_mat1 <- co_occur_mat[inv, inv]
co_occur_mat1[1:7, 1:7]
noIdx <- which(colnames(co_occur_mat1) %in% c("해", "후", "한", "의", "이", "장", "저", "적", '전', '제', '주', '중', '지',
'은', '을', '위', '월', '원', '세', '수', '로', '만', '명', '본', '분', '라', '데', '도', '두',
'들', '를', '기', '나', '날', '내', '대', '데', '개', '그', '때', '리', '화', '양', '들이',
'듯', '과', '드', '니', '바', '림', '얼', '거', '시', "호", "년", "것", "출처", "번", "속",
'amp', 'cm', 'com', 'gt', 'https', 'k', 'lt', 'm',
'q', 's', 'x', 'v', 'www', 'u', 'a', 'b', 'r',
'ne', 'l', 'e', 'd'))
co_occurrence <- co_occur_mat1[-noIdx, -noIdx]
which_are_related <- data.frame()
for (i in 1:nrow(co_occurrence)){
for (j in 1:ncol(co_occurrence)){
if (i < j){
if (co_occurrence[i,j] > 30){
row = i
col = j
row_word = rownames(co_occurrence)[i]
col_word = colnames(co_occurrence)[j]
dat <- data.frame("row" = row, "col" = col, "row_word" = row_word, "col_word" = col_word, "cnt" = co_occurrence[i,j])
which_are_related <- rbind(which_are_related, dat)
}
}
}
}
head(which_are_related)
which_are_related %>%
arrange(cnt)
library(tidyverse)
which_are_related %>%
arrange(cnt)
?arrange
which_are_related %>%
arrange(desc(cnt))
which_are_related %>%
arrange(desc(cnt)) %>%
head(20)
which_are_related %>%
arrange(desc(cnt)) %>%
head(50)
