---
title: "Final"
author: "Hyun, Heo"
date: "2018<eb>년 9<ec>월 9<ec>일"
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

# setting

```{r}
library(tidyverse)
library(car)
library(boot)

dat <- read.csv("mobility.csv")
apply(is.na(dat), 2, sum)
dat$State <- as.factor(dat$State) # factorization
dat$Urban <- as.factor(dat$Urban) # factorization
dat$Violent_crime[is.na(dat$Violent_crime)] <- 0
dat <- dat %>%
  filter(!is.na(Mobility))
```

na가 각 컬럼별로 얼마나 있는지 확인하고, Violent_crime이 NA인 것은 0으로, Mobility가 NA인 것은 제거했습니다.

# 1

## a
```{r}
ggplot(dat) + geom_point(aes(x = Longitude, y = Latitude, color = Mobility)) + 
  scale_color_gradient()
```


## b
```{r}
dat %>%
  mutate(MobilityCat = ifelse(Mobility > 0.1, "high", "low")) %>%
  filter(State != "HI", State != "AK") %>%
  ggplot() + geom_point(aes(x = Longitude, y = Latitude, color = MobilityCat))+ 
  scale_color_grey(start=0.1, end = 0.7)
```

경향성이 좀 더 잘 보인다. 대체적으로 서부가 더 이동성 높다.

# 2

## a : Population
```{r}
lm.fit <- lm(Mobility~Population, data=dat)
ggplot(dat) + geom_point(aes(x=Population, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients # need log scale
```

로그 스케일 적용 등의 변형이 필요해 보인다.


## b : Mean household income per capita
```{r}
lm.fit <- lm(Mobility~Income, data=dat)
ggplot(dat) + geom_point(aes(x=Income, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients
```


## c : Racial segregation
```{r}
lm.fit <- lm(Mobility~Seg_racial, data=dat)
ggplot(dat) + geom_point(aes(x=Seg_racial, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients
```


## d : Income share of the top 1%
```{r}
lm.fit <- lm(Mobility~Share01, data=dat)
ggplot(dat) + geom_point(aes(x=Share01, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients # need log scale or remove outlier
```

로그 스케일 변화나 아웃라이어 제거 등을 할 필요가 있다.

## e : Mean school expenditures per pupil
```{r}
lm.fit <- lm(Mobility~School_spending, data=dat)
ggplot(dat) + geom_point(aes(x=School_spending, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients
```


## f : Violent crime rate
```{r}
lm.fit <- lm(Mobility~Violent_crime, data=dat)
ggplot(dat) + geom_point(aes(x=Violent_crime, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients # discrete
```

Violent_crime이 이산형으로 나오는데, 그것에 대한 이유는 알 수 없지만, 이산형이기 때문에 회귀직선이 명확하지 않다.

## g : Fraction of workers with short commutes
```{r}
lm.fit <- lm(Mobility~Commute, data=dat)
ggplot(dat) + geom_point(aes(x=Commute, y=Mobility)) + 
  geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color = "pink", size = 2)
lm.fit$coefficients
```


# 3

## a
```{r}
lm_appro <- lm(Mobility~.-ID-Name-State-Longitude-Latitude-HS_dropout-Student_teacher_ratio
               , data=dat) # not appropriate or NA
# summary(lm_appro)
coeffs <- summary(lm_appro)$coef[,1:2]
coeffs
```


## b
ID 변수는 식별을 위해 필요하지만 회귀 등의 방법에 사용하기에는 담고 있는 정보가 무의미해서 사용하지 않는다. 만약 사용한다면 굉장히 잘못된 결과가 나올 것이다.

## c
더 많은 변수를 제거했어야 했다고 생각하지만 stepwise 방식을 쓰지 않고 필요하다고 생각되는 변수를 넣으려고 하니 쉽게 변수를 제거할 수 없었다. 점차적으로 최적화 해나가는 과정이 있을 것이라 생각하고 최소한의 변수만 제거하려고 했다. 이 때 제거한 변수는 b에서 설명한 ID, ID와 비슷한 역할을 하는 Name과 State, 지도를 그릴 때에는 유용하지만 회귀의 독립변수라 할 수 없는 위경도, NA 값이 높은 HS_dropout과 Student_teacher_ratio를 제거했다.

## d
```{r}
coeffs[c("Population", "Income", "Seg_racial", "Share01", "School_spending", "Violent_crime", "Commute"),]
```

계수가 변했다. 부호가 바뀐 경우도 있다. 이유는 변수간의 공분산 혹은 공선성이다.

## e
```{r}
vif_appro <- vif(lm_appro)
vif_appro[vif_appro > 10] # ridge or variable elimination
```

VIF가 10 초과인 7개의 변수가 나왔다. 대부분 10뿐 아니라 100 이상이라는 점에서 다중공선성이 매우 의심된다. 해결책으로는 Ridge regression, PCA 등이 있고 직접 해당 변수들을 제거하는 방법도 있다. 


# 4

## a
apply(is.na(dat[,c("Colleges", "Tuition", "Graduation")]), 2, sum)

## b

```{r}
ggplot(dat) + geom_point(aes(x=Population, y=Mobility, color = Colleges)) + 
  scale_color_gradient(na.value = "red")
ggplot(dat) + geom_point(aes(x=Population, y=Mobility, color = Tuition)) + 
  scale_color_gradient(na.value = "red")
ggplot(dat) + geom_point(aes(x=Population, y=Mobility, color = Graduation)) + 
  scale_color_gradient(na.value = "red")
```

인구수가 0에 굉장히 가까울 때 각 값이 결측되어있다. 따라서 결측을 0으로 생각해도 무방하다.

## c
```{r}
dat[,c("Colleges", "Tuition", "Graduation")][is.na(dat[,c("Colleges", "Tuition", "Graduation")])] <- 0
dat <- dat %>%
  mutate(HE = ifelse(Colleges == 0 & Tuition == 0 & Graduation ==0, 0, 1))
```


# 5

```{r}
dat <- dat %>%
  mutate(logMobil=log(Mobility))
flm.fit <- lm(logMobil~Population+Seg_poverty+Commute+Gini+Manufacturing+
                Religious+Violent_crime+Single_mothers+Progressivity,
              data=dat)
# summary(flm.fit)
fcoeff <- summary(flm.fit)$coef
fcoeff[,1:2]
```

저는 Mobility 변수를 log화 한 변수를 새로 만들어주었습니다. 설명변수는 되도록 NA가 없는 변수 위주로 활용했습니다. 여러 값을 넣었다 빼면서 다중공선성이 크지 않으면서 유의미한 변수들을 선택했습니다. 


# 6

```{r}
dat$logpred <- predict(flm.fit)
dat$pred <- exp(dat$logpred)
ggplot(dat) + geom_point(aes(x = Longitude, y = Latitude, color = pred)) + 
  scale_color_gradient()

ggplot(dat) + geom_point(aes(x = Longitude, y = Latitude, color = Mobility)) + 
  scale_color_gradient()
```

먼저 나온 그림이 예측에 의한 것이고, 나중에 나온 그림이 실제 값을 나타냅니다. 비슷하게 나타나는 것을 보아 어느정도 잘 예측한 것으로 생각됩니다.


# 7

## a

```{r}
Pitts <- dat %>%
  filter(State == "PA")

Pitts %>%
  select(Mobility, pred)
```

## b

```{r}
bind_cols(double = Pitts$pred * exp(fcoeff["Violent_crime",1]*Pitts$Violent_crime),
          halve = Pitts$pred / exp(fcoeff["Violent_crime",1]*Pitts$Violent_crime/2))
```

## c

```{r}
exp(predict(flm.fit, Pitts[,names(flm.fit$coefficients)[-1]], level=0.95, interval="confidence"))
```


## d

```{r}
exp(predict(flm.fit, Pitts[,names(flm.fit$coefficients)[-1]], level=0.95, interval="prediction"))
```

confidence interval은 주어진 데이터를 이용해서 각 데이터 포인트에서 모수의 값을 추정할 때 95%의 확률로 있을 구간을 정하는 것이고, prediction interval은 모수가 아니라 후에 들어올 수 있는 값의 범위를 95%의 확률로 정하는 것이기 때문에 목적이 다르고, 이런 이유로 prediction interval이 범위가 더 넓다.


# 8

## a

```{r}
dat <- dat %>%
  mutate(residual = Mobility - pred,
         logres = logMobil - logpred)

g <- ggplot(dat) + geom_point(aes(x = Longitude, y = Latitude, color = residual))
g
```

## b

```{r}
top5 <- dat %>% 
  arrange(desc(residual)) %>%
  head(5) %>%
  mutate(group = 1)

bottom5 <- dat %>% 
  arrange(residual) %>%
  head(5) %>%
  mutate(group = 2)

tb5 <- bind_rows(top5, bottom5)
g + 
  geom_label(data=tb5, aes(x = Longitude, y = Latitude, label=Name, fill = factor(group)), size = 5, parse = TRUE)
```


# 9

## a

```{r}
ggplot(dat, aes(x = Mobility, y = pred)) + geom_point() + geom_smooth(method = "lm")
ggplot(dat, aes(x = logMobil, y = logpred)) + geom_point() + geom_smooth(method = "lm")
```

첫번째 그림은 x,y 모두 로그화하지 않은 그림이고 두번째 그림은 로그화 한 그림이다. 로그화 한 경우 선형의 관계가 있음이 확실하다. 즉 잘 예측했다.

## b
```{r}
ggplot(dat, aes(x = Mobility, y = residual)) + geom_point() + geom_smooth(method = "lm")
ggplot(dat, aes(x = logMobil, y = logres)) + geom_point() + geom_smooth(method = "lm")
```

첫번째 그림은 x,y 모두 로그화하지 않은 그림이고 두번째 그림은 로그화 한 그림이다. 로그화 한 경우 약간의 경향성은 남아있지만 비교적 관계가 없다. 잔차가 특정 구간에서 다르지 않게 모델링을 잘 했다고 볼 수 있다.


# 10

## a

```{r}
mmdat <- dat[,c("Middle_class", "Mobility")]
mmdat <- mmdat %>% filter(!is.na(Middle_class))
nlm.fit <- lm(Mobility~Middle_class, data = mmdat)
summary(nlm.fit)
ggplot(mmdat, aes(Middle_class, Mobility)) + geom_point() + 
  geom_abline(intercept = nlm.fit$coefficients[1], slope = nlm.fit$coefficients[2], color = "red")
interval_conf <- predict(nlm.fit, interval = "confidence")
interval_pred <- predict(nlm.fit, interval = "prediction")
mmdatinter <- bind_cols(mmdat, as.data.frame(interval_conf), pred_lwr = interval_pred[,2], pred_upr = interval_pred[,3])
ggplot(mmdatinter, aes(Middle_class, Mobility)) + geom_point() + 
  geom_line(aes(y=fit), color = "red") +
  geom_line(aes(y=upr), color = "blue") +
  geom_line(aes(y=lwr), color = "blue") +
  geom_line(aes(y=pred_upr), color = "green") +
  geom_line(aes(y=pred_lwr), color = "green")
```

confidence interval은 파란색, prediction interval은 초록색으로 그렸습니다. 그림이 Middle_class가 커지면서 residual의 분산이 커지면서 N(0,2) 꼴은 잘 맞지 않습니다. 

## b

```{r}
boot.conf <- function(data, indices){
  data <- data[indices,]
  lm.fit <- lm(Mobility~Middle_class, data = data)
  interval_conf <- predict(lm.fit, interval = "confidence")
  mean(interval_conf)
}
boot.out <- boot(mmdat, boot.conf, 100)
boot.out
boot.ci(boot.out, type = c("norm", "perc"))

boot.pred <- function(data, indices){
  data <- data[indices,]
  lm.fit <- lm(Mobility~Middle_class, data = data)
  interval_pred <- predict(lm.fit, interval = "prediction")
  mean(interval_pred)
}
boot.out <- boot(mmdat, boot.pred, 100)
boot.out
boot.ci(boot.out, type = c("norm", "perc"))
```

먼저 나온 값이 confidence interval을 resampling을 통해 추정한 것이고,
다음 나온 값이 prediction interval을 resampling을 통해 추정한 것이다.

## c

```{r}
ssfit <- smooth.spline(mmdat$Middle_class, mmdat$Mobility, cv = TRUE)
ssfit

boot.ss <- function(data, indices){
  data <- data[indices,]
  ssfit <- smooth.spline(data$Middle_class, data$Mobility, df = 5.056275)
  mean(ssfit$y)
}
ssboot <- boot(mmdat, boot.ss, 100)
ssboot
boot.ci(ssboot, type = c("norm", "perc"))
ssci <- boot.ci(ssboot, type = c("perc"))$percent[c(4,5)]
ssci

ssdf = bind_cols(x=ssfit$x, y=ssfit$y, lwr = ssfit$y - ssci[1], upr = ssfit$y + ssci[2])

ggplot(ssdf, aes(x, y)) + geom_point() + geom_line(aes(y=lwr), color = "blue") +
  geom_line(aes(y=upr), color = "green")
```

## d

점점 올라가는 형태를 잘 표현할 수 있기 때문에 smoothing이 필요했다.